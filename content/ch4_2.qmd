# Tests du $\chi_2$


\newcommand{\1}{\mathbf{1}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\bp}{\mathbf{p}}


Les tests du $\chi_2$ sont une famille de tests qui visent, pour la plupart, à tester si un échantillon discret a été généré par une loi précise ; on parle parfois de test d'ajustement. 


## Loi multinomiale

Soit $\Omega$ un ensemble fini à $k$ éléments, disons pour simplifier $\{1, \dotsc, k\}$. On notera $S_k$ l'ensemble des lois de probabilités sur cet ensemble, c'est-à-dire les $\mathbf{p} = (p_1, \dotsc, p_k)$ tels que les $p_i$ sont positifs et de somme 1. On observe $n$ tirages iid selon une même loi sur $\Omega$. Formellement, le modèle statistique est donné par $(\mathbf{p}^{\otimes n} : \mathbf{p} \in S_k)$. 



On note $N_j$ le nombre d'observations égales à $j$. Le vecteur $N=(N_1, \dotsc, N_k)$ suit une loi multinomiale de paramètres $n$ et $\mathbf{p}$, donnée par 
\begin{align*}
\PP(N = (n_1, \dotsc, n_k)) = \frac{n!}{n_1! \dotsc n_k!} \prod_{j=1}^k {p}_j^{n_j},
\end{align*} 
où $\sum_{j=1}^k n_j = n$. Cette loi sera notée $\mathrm{Mult}(n, \mathbf{p})$. 

:::{#thm-cvloimult}

Soit $N \sim \mathrm{Mult}(n,\mathbf{p})$. Le vecteur $\sqrt{n}(\frac{N}{n}- \mathbf{p})$ converge en loi vers $N(0, \Sigma)$, où 
$$ \Sigma = \mathrm{diag}(\mathbf{p}) - \mathbf{p}\mathbf{p}^\top.$${#eq-covarmultin}

::: 

:::{.proof}

On commence par remarquer que $N = \sum_{i=1}^n Z_i$, où $Z_i=(\1_{X_i=1}, \dotsc, \1_{X_i=k})$. Les $Z_i$ sont iid de moyenne $\mathbf{p}$. Les covariances des entrées $i$ et $j$ de $Z_k$ sont données par 
$$\mathbb{E}[\mathbf{1}_{X_k=i}\mathbf{1}_{X_k=j}] - p_i p_j = \delta_{i,j}p_i - p_i p_j,$$
ce qui montre que la matrice de covariance des $Z_k$ est @eq-covarmultin. Il suffit alors d'appliquer le TCL. 

:::


**Remarque.** On considère que cette approximation normale est correcte dès que $\mathbb{E}[N_j]$ est plus grand que $5$ pour tout $j$. 

## Test d'adéquation 

Le test du $\chi^2$ d'adéquation consiste à tester l'hypothèse nulle $$H_0: \mathbf{p}= \mathbf{p}_0$${#eq-chi2nul} contre l'hypothèse alternative
$$H_1:\mathbf{p} \neq \mathbf{p}_0,$${#eq-chi2alt}
pour une valeur de $\mathbf{p}_0$ fixée au préalable. À partir de maintenant, on supposera implicitement que toutes les entrées de $\bp_0$ sont non nulles --- cela garantira que les limites en loi trouvées ci-dessous ne sont  pas dégénérées.  


:::{#exm-testadeq}

On peut se demander si, dans la langue courante, les 21 lettres de l'alphabet ont à peu près la même probabilité d'apparaître comme première lettre d'un mot. Cela revient à tester si $\bp_0=(1/26, \dotsc, 1/26)$, hypothèse qui est évidemment fausse, il suffit de regarder l'épaisseur des 26 sections du dictionnaire pour s'en rendre compte. 

Qu'en est-il des 9 chiffres ? On peut vouloir tester si, dans n'importe quel document (journal, site internet, article scientifique), ces 9 chiffres apparaissent à peu près uniformément en tant que premier chiffre d'un nombre. Cela reviendrait à tester $\bp_0 = (1/9, \dotsc, 1/9)$. 

Ce n'est pas le cas et cette hypothèse est très fréquemment réfutée : le premier chiffre significatif d'un nombre est bien plus souvent 1 ($\approx 30\%$ des cas) que $9$ ($\approx 5\%$ cas). Ce phénomène s'appelle [*loi de Benford*](https://fr.wikipedia.org/wiki/Loi_de_Benford). 

:::

Le théorème @thm-cvloimult dit que $\sqrt{n}(\frac{N}{n}- \mathbf{p}) \approx N(0, \Sigma)$. Notons $\sqrt{\bp_0} = (\sqrt{p_1}, \dotsc, \sqrt{p_k})$ et $D = \mathrm{diag}(\sqrt{\bp}_0)$. Sous $H_0$,  $D^{-1} \sqrt{n}(\frac{N}{n}- \mathbf{p}_0)$ converge en loi vers  $D^{-1}N(0,\Sigma) = N(0,D^{-1}\Sigma (D^{-1})^\top)$. Que vaut cette matrice de covariance ? 

D'abord, comme $D$ est diagonale, $D^{-1}$ l'est aussi et $(D^{-1})^\top$ vaut $D^{-1}$. De plus, $D^2$ est égal à $\mathrm{diag}(\bp_0)$. Enfin, en faisant la multiplication on voit vite que $D^{-1}\bp_0 = \sqrt{\bp}_0$. Ainsi, on voit que $D^{-1}\Sigma D^{-1}$ vaut également $D^{-1}D^2 D^{-1} - D^{-1}\bp_0 \bp_0 D^{-1}$ c'est-à-dire $$I_k - \sqrt{\bp_0} \sqrt{\bp_0}^{\top}.$$
L'appendice @sec-al rappelle pourquoi cette matrice est une matrice de projection orthogonale. 


On a montré que $D^{-1}\sqrt{n}(N/n - \bp_0)$ converge en loi vers
$$N(0, I_k - \sqrt{\mathbf{p}_0} \sqrt{\mathbf{p}_0}^\top).$$
La statistique qui va nous servir à faire des tests est la norme au carré de $D^{-1}\sqrt{n}(N/n - \bp_0)$. En manipulant cette expression, on obtient sa forme usuelle, le *contraste du $\chi_2$*. 


:::{#def-contraste}

## Contraste du $\chi_2$

Dans le contexte ci-dessus, le *contraste du $\chi_2$* associé à la loi $\mathbf{p}$ est la statistique

$$ D_n(\mathbf{p}) = \sum_{j=1}^k \frac{(N_j - n{p}_j)^2}{n{p}_j}.$$
:::

Pour faire des tests, il suffit donc de trouver la loi asymptotique de cette statistique. 

:::{#thm-loicontraste}

Sous l'hypothèse nulle @eq-chi2nul, la statistique $D_n$ converge en loi vers $\chi_2(k-1)$. De plus, sous l'hypothèse alternative @eq-chi2alt, $D_n$ tend vers $+\infty$ presque sûrement. 
:::


:::{.proof}

Comme $|\sqrt{\bp_0}|$ vaut 1, la matrice $\pi_0=I_k -\sqrt{\mathbf{p}_0} \sqrt{\mathbf{p}_0}^T$ est la matrice de projection sur l'orthogonal du vecteur $\sqrt{\mathbf{p}_0}$ (je vous renvoie à l'appendice @sec-al). Le théorème de Cochran (@thm-cochran) implique alors que la statistique $D_n$, qui est égale à 
$$
\left| \mathrm{diag}(1/\sqrt{\mathbf{p}_0}) \sqrt{n}\left(\frac{N}{n}- \mathbf{p}_0\right) \right |^2, $${#eq-contraste1}
converge en loi vers la norme de la projection d'une gaussienne $N(0,I_k)$ sur un sous-espace de dimension $k-1$, c'est-à-dire une loi $\chi_2(k-1)$. Sous l'hypothèse alternative, il y a au moins un $p_i$ non nul tel que $p_i \neq (p_0)_i$. Ainsi, @eq-contraste1 est plus grand que $n(N_i/n - (p_0)_i)^2 / p_i$, mais $N_i$ suit une loi $\mathrm{Bin}(n,p_i)$ et donc $N_i / n$ converge en probabilité vers $p_i$. Il est alors clair que $n(N_i/n - (p_0)_i)$ converge vers $+\infty$.

:::

Un test de niveau $1-\alpha$ pour l'hypothèse @eq-chi2nul est alors donné par la région de rejet
$$ \{ D_n(\mathbf{p}_0) > \kappa_{k-1, 1-\alpha} \}$$

 où $\kappa_{k-1, 1-\alpha}$ est le quantile d'ordre $1-\alpha$ d'une $\chi^2(k-1)$. Si $\bp$ n'est pas égal à $\bp_0$, le contraste $D_n$ tend vers l'infini, donc le test sera forcément dans la zone de rejet : si l'hypothèse alternative est simple, la puissance du test tend donc vers 1. 


## Test d'indépendance

Les tests du $\chi_2$ d'indépendance sont omniprésents en sciences humaines. Dans ces tests, on observe des variables aléatoires qui sont des *couples* à valeur dans deux espaces discrets ; disons, pour simplifier, que cet espace est $\Omega = \{1, \dotsc, k\}\times \{1, \dotsc, h\}$. Les observations $(x_i, y_i)$ sont des réalisations d'une variable aléatoire $(X,Y)$. 


:::{#exm-test}

On récolte des données sur le groupe socio-professionnel (GSP) et le genre. Chaque observation correspond à une personne, possédant deux attributs : $\mathtt{genre}$, valant 0 ou 1, et $\mathtt{GSP}$, valant l'une des 6 groupes définis par l'INSEE (Agriculteur, artisan, cadre, etc.). On cherche à déterminer si les deux modalités sont indépendantes, c'est-à-dire si la proportion d'hommes et de femmes dans chaque groupe ne diffère pas significativement en fonction du groupe. 

:::


Ici, le modèle statistique sera donc $(\mathbf{p}^{\otimes n} : \mathbf{p} \in S_{k,h})$, où $S_{k,h}$ est l'ensemble des $\mathbf{p} = (p_{i,j})$ qui sont des lois de probabilité. 

Si $\mathbf{p}$ est la loi de $(X,Y)$, alors $X$ et $Y$ sont indépendantes si et seulement si $\mathbf{p}$ peut s'écrire sous la forme $p_{i,j} = p^x_i p^y_j$, où $\mathbf{p}^x \in S_k$ et $\mathbf{p}^y \in S_h$. L'ensemble de ces lois sera noté $I_{k,h}$ (« I » pour « Indépendant » ). Les tests d'indépendance visent à tester l'hypothèse nulle 
$$ H_0 : \mathbf{p}\in I_{k,h}$${#eq-chi2indepnul}
contre l'hypothèse alternative 
$$ H_1 : \mathbf{p} \notin I_{k,h}.$$


La procédure pour effectuer un tel test nécessite plusieurs étapes. 

Si $\mathbf{p}$ était effectivement la loi de deux variables indépendantes $\bp^x$ et $\bp^y$, alors ses marginales seraient précisément $\bp^x$ et $\bp^y$, que l'on pourrait facilement estimer. Pour chaque $i$ et chaque $j$, les estimateurs $\hat{\bp}^x$ et $\hat{\bp}^y$ définis par
$$\hat{p}^x_i = \frac{\sum_{j=1}^h N_{i,j}}{n}$$
et 
$$\hat{p}^y_j = \frac{\sum_{i=1}^k N_{i,j}}{n}$$
sont effectivement des estimateurs sans biais et convergents des quantités $p^x_i, p^y_j$. De plus, sous l'hypothèse nulle, $\hat{p}^x_i \hat{p}^y_i$ serait effectivement un estimateur convergent de $p_{i,j}$. 

De plus, si $\bp$ était effectivement de la forme $\hat{\bp}^x\hat{\bp}^y$, alors la moyenne théorique des éléments de classe $(i,j)$ serait $n\hat{p}^x_i \hat{p}^y_j$. Cette quantité, notée $\check{N}_{i,j}$, s'appelle *effectif théorique*. Nous pouvons maintenant construire la statistique qui nous servira à tester tout cela. 

:::{#def-statpearson}

## Statistique de Pearson

La statistique de Pearson est définie par 

$$C_n = \sum_{i=1}^k \sum_{j=1}^h \frac{(N_{i,j} - \check{N}_{i,j})^2}{\check{N}_{i,j}}. $$

:::

Cette statistique possède une loi limite connue, encore en vertu du théorème de Cochran. Noter que la statistique de Pearson possède une expression alternative, 
$$C_n = \sum\sum \frac{n(\hat{p}_{i,j} - \hat{p}^x_i \hat{p}^y_j)^2}{\hat{p}^x_i \hat{p}^y_j}.  $$

:::{#thm-testindepchi2}

## Loi de la statistique de Pearson

Sous l'hypothèse nulle @eq-chi2indepnul, $C_n$ converge en loi vers 
$$ \chi_2((k-1)(h-1)).$$
De plus, pour n'importe quelle loi $\bp_1$ qui n'est pas dans $I_{k,h}$, $C_n \to +\infty$ presque sûrement. 
:::

:::{.proof}
C'est une conséquence un peu plus technique du théorème de Cochran. 

:::

Tout cela permet encore une fois d'obtenir des tests : en abrégeant $\kappa_{1 - \alpha} = \kappa_{(k-1)(h-1), 1-\alpha}$, on obtient que $\mathbb{P}(C_n > \kappa_{1-\alpha}) \to \alpha$. Ainsi, la région de rejet 
$$\{C_n > \kappa_{1-\alpha}\} $$
fournit un test de niveau asymptotique $1-\alpha$. La seconde partie du théorème dit que si la véritable loi sous-jacente n'est effectivement pas la loi de deux variables indépendantes, alors ce test sera systématiquement rejeté --- autrement dit, si l'hypothèse alternative est simple, la puissance de ce test tend vers 1. 

<!-- 
## Test d'homogénéité

On suppose cette fois-ci que l'on observe à la fois $X_1, \dotsc, X_n$ et $Y_1, \dotsc, Y_m$, les deux étant iid. La question que l'on se pose est celle de l'égalité en loi des $Y$ et des $X$. 

Une manière de s'en sortir est de passer par un test du $\chi^2$ d'indépendance (plus tard). On peut aussi passer par un test du $\chi^2$ d'appartenance à un sous-espace. Pour se faire, on dissocie les deux espaces d'arrivée pour obtenir $[\![1,2k]\!]$, et on regroupe les observations en supposant que l'on observe $\tilde{X}_1, \dotsc, \tilde{X}_{n+m}$ i.i.d. à valeurs dans $[\![1,2k]\!]$, où les $n$ premières observations correspondent aux $X_i$, les $m$ dernières aux $Y_i+k$. 

Avec ce bidouillage, on a, pour $1 \leq j \leq k$, $\P(\tilde{X}_i=j) = \frac{n}{n+m} \P(X_1=j)$, et $\P(\tilde{X}_i = j) = \frac{m}{n+m} \P(Y_1=j)$ si $k+1\leq j \leq 2k$. Si on note $Z_i = (\1_{\tilde{X}_i=j})_{j=1, \dotsc, 2k}$, $Z_i$ suit une loi multinomiale $\mathcal{M}(n+m, (\frac{n}{n+m} \mathbf{p}_X, \frac{m}{m+n} \mathbf{p}_Y))$. On supposera dans la suite que toutes les proportions sont non nulles. 

En notant les proportions $\alpha_n = \frac{n}{n+m}$, $\alpha_m = \frac{m}{m+n}$ et $\mathbf{p}_{\tilde{X}}$ le vecteur moyenne de $\tilde{X}$, on a, sous $H_0$, 
\begin{align*}
A \mathbf{p}_{\tilde{X}} = 0_k,
\end{align*}
où $A$ est la matrice $(\mathrm{diag}(\alpha_n^{-1},k) | \mathrm{diag}(-\alpha_m^{-1},k)) \in M_{k,2k}$. Toujours sous $H_0$, on en déduit alors
\begin{align*}
A(Z_1-\mathbf{p}_{\tilde{X}}) = AZ_1.
\end{align*}
Une application du théorème central limite donne alors
\begin{align*}
\sqrt{n+m} A \bar{Z}_{n+m} \leadsto \mathcal{N}(0_k, A \Sigma A^T),
\end{align*}
avec $\Sigma = \mathrm{diag}(\mathbf{p}_{\tilde{X}}) - \mathbf{p}_{\tilde{X}} \mathbf{p}_{\tilde{X}}^T$. Comme $A \mathbf{p}_{\tilde{X}} = 0_k$, un calcul matriciel donne
\begin{align*}
A \Sigma A^T = A \mathrm{diag}(\mathbf{p}_{\tilde{X}}) A^T = (\alpha_n^{-1} + \alpha_m^{-1}) \mathrm{diag}(\mathbf{p}) \in M_{k,k},
\end{align*}
où $\mathbf{p} = \mathbf{p}_{X} = \mathbf{p}_Y$ (rappelons que l'on travaille sous $H_0$).

On en déduit alors que, sous $H_0$, 
\begin{align*}
\mathrm{diag}(\mathbf{p})^{-\frac{1}{2}} \frac{A\bar{Z}_{n+m}}{\sqrt{\frac{1}{n} + \frac{1}{m}}} \leadsto \mathcal{N}(0,I_k).
\end{align*}
On peut estimer $\mathrm{diag}(\mathbf{p})$ via $\frac{1}{n+m}  (N_{j,X} + N_{j,Y})_{j=1, \dotsc, k}$, où $N_{j,X} = \sum_{i=1}^n \1_{X_i=j}$ et $N_{j,Y} = \sum_{i=1}^m \1_{Y_i=j}$. La convergence en proba étant assurée par la loi des grands nombres, le lemme de Slustsky donne alors en prenant la norme au carré
\begin{align*}
\sum_{j=1}^k \frac{nm \left ( \frac{N_{j,X}}{n} - \frac{N_{j,Y}}{m} \right )^2}{N_{j,X} + N_{j,Y}} \leadsto \chi^2(k),
\end{align*}
sous $H_0$. Ce dont on peut déduire un test de niveau $\alpha$.

-->

