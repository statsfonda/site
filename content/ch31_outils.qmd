
# Intervalles non-gaussiens


Les outils de base pour construire des intervalles de confiance dans des circonstances générales (non gaussiennes) sont les inégalités de concentration. Une inégalité de concentration pour une variable aléatoire intégrable $X$ consiste à borner $\mathbb{P}(|X - \mathbb{E}[X]|>x)$ par quelque chose de petit quand x est grand : on cherche à contrôler la probabilité pour que les réalisations de la variable aléatoire $X$ soient éloignées de leur valeur moyenne $\mathbb{E}[X]$ de plus de $x$. 


## Inégalités de concentration

L’inégalité de concentration la plus élémentaire est l’inégalité de Markov. Si $X$ est une variable aléatoire positive, alors il est évident que pour n’importe quel nombre $a$, on a $a \mathbf{1}_{X>a} \leqslant X \mathbf{1}{X>a}$, donc en intégrant cette relation on obtient $a \mathbb{P}(X>a)\leqslant \mathbb{E}[X \mathbf{1}_{X>a}]\leqslant \mathbb{E}[X]$. L’inégalité de Bienaymé-Chebychev est un raffinement. 


### Inégalité de Bienaymé-Tchebychev


:::{#thm-bt}

Soit $X$ une variable aléatoire de carré intégrable. Alors, 
$$ \mathbb{P}(|X - \mathbb{E}[X]|\geqslant x)\leqslant \frac{\mathrm{Var}(X)}{x^2}.$$ 

:::



:::{.proof} 
Élever au carré les deux membres de l'inégalité dans $\mathbb{P}$, puis appliquer l'inégalité de Markov à la variable aléatoire positive $|X - \mathbb{E}X|^2$ dont l'espérance est $\mathrm{Var}(X)$. 
:::

### Inégalité de Hoeffding


:::{#thm-hoeffding}

## Inégalité de Hoeffding

Soient $X_1, \dotsc, X_n$ des variables aléatoires indépendantes, pas forcément de même loi. On suppose que chaque $X_i$ est à valeurs dans un intervalle borné $[a_i, b_i]$ et on pose $S_n = X_1 + \dotsc + X_n$. Pour tout $t>0$, 

$$\mathbb{P}(S_n - \mathbb{E}[S_n] \geqslant t) \leqslant e^{-\frac{2t^2}{\sum_{i=1}^n(b_i - a_i)^2}}$${#eq-hoeffding}
et
$$\mathbb{P}(|S_n - \mathbb{E}[S_n]| \geqslant t) \leqslant 2e^{-\frac{2t^2}{\sum_{i=1}^n(b_i - a_i)^2}}. $${#eq-hoeffdingsym}


:::

La démonstration se fonde sur le lemme suivant. 

:::{#lem-hoeffding}

## lemme de Hoeffding

Soit $X$ une variable aléatoire à valeurs dans $[a,b]$. Pour tout $t$, 

$$\mathbb{E}[e^{t(X-\mathbb{E}[X]}] \leqslant e^{\frac{t^2(b-a)^2}{8}}.$${#eq-lemme_hoeffding}

:::

:::{.proof}
Soit $X$ une variable aléatoire, que par simplicité on supposera centrée et à valeurs dans l'intervalle $[a,b]$ ($a$ est forcément négatif). En écrivant 
$$x = a\times \frac{b-x}{b-a} + b\times \left(1 - \frac{b-x}{b-a}\right)$$
 et en utilisant la convexité de la fonction $x \mapsto e^{tx}$, on obtient $e^{tX}\leqslant (b-X)e^{ta}/(b-a) + (1 - (b-x)/(b-a)) e^{bt})$, puis en prenant l'espérance et le fait que $X$ est centrée et en simplifiant, 
 $$\mathbb{E}[e^{tX}]\leqslant \frac{be^{ta} - ae^{tb}}{b-a}.$$ Notons $f(t)$ le terme à droite ; pour montrer @eq-lemme_hoeffding, il suffit de montrer que $\ln f(t) \leqslant t^2(b-a)^2/8$. La formule de Taylor dit que 
 $$ \ln f(t) = \ln f(0) + t (\ln f)'(0) + \frac{t^2}{2}(\ln f)''(\xi)$$
 pour un certain $\xi$. Or, $\ln f(0) = \ln 1 = 0$, $(\ln f)'(0) = f'(0)/f(0) = 0$, et il suffit donc de montrer que $(\ln f)''(t)$ est toujours plus petit que $(b-a)^2/4$ pour conclure. Un simple calcul montre que $\ln f(t) = \ln(b/(b-a)) + ta + \ln(1 - ae^{t(b-a)} / b)$, et donc
 $$ (\ln f)''(t) = \frac{(a/b)(b-a)e^{t(b-a)}}{(1 - ae^{t(b-a)}/b)^2}.$$
 L'inégalité $uv/(u-v)^2 \leqslant 1/4$ appliquée à $u = a/b$ et $v = e^{t(b-a)}$ permet alors de conclure. 
:::


*Preuve de l'inégalité de Hoeffding.*
En remplaçant $X_k$ par $X_k - \mathbb{E}[X_k]$, on peut supposer que tous les $X_i$ sont centrés et étudier seulement $\mathbb{P}(S_n >t)$. 
Écrivons $\mathbb{P}(S_n > t) = \mathbb{P}(e^{\lambda S_n} > e^{\lambda t})$, où $\lambda$ est un nombre positif que l'on choisira plus tard. L'inégalité de Markov borne cette probabilité par $\mathbb{E}[e^{\lambda S_n}]e^{-\lambda t}$. Comme les $X_i$ sont indépendantes, $\mathbb{E}[e^{tS_n}]$ est le produit des $e^{ \varphi_k(\lambda)}$ où $\varphi_k(t) = \ln \mathbb{E}[e^{itX_k}]$. En appliquant le lemme de Hoeffding à chaque $\varphi_k$, on borne $\mathbb{P}(S_n >t)$ par 
$$ \exp\left(\sum_{i=1}^n \frac{(b_i - a_i)^2 \lambda^2}{8} - t\lambda\right).$$
Le minimum en $\lambda$ du terme dans l'exponentielle est atteint au point $4t / \sum (a_i - b_i)^2$ et la valeur du minimum est le terme dans l'exponentielle de @eq-hoeffding. On déduit @eq-hoeffdingsym par une simple borne de l'union.  



La démonstration de l'inégalité de Hoeffding ne dépend pas directement du fait que $X$ est bornée, mais plutôt de @eq-lemme_hoeffding. Toutes les variables aléatoires qui vérifient une inégalité de type $\mathbb{E}[e^{tX}]\leqslant e^{c t^2}$ pour une constante $c$ peuvent donc avoir leur propre inégalité de Hoeffding. 


## Intervalles de confiance non-gaussiens

Les inégalités de concentration servent à construire des intervalles de confiance dans de nombreux cas où il est difficile de connaître exactement la loi de la statistique de test. Dans cette partie, on développe plusieurs exemples. 


### Estimation du paramètre $p$ dans un modèle de Bernoulli. {#sec-icber}

Soient $X_1, \dotsc, X_n$ des variables indépendantes de loi $\mathscr{B}(p)$, dont on cherche à estimer le paramètre $p\in ]0,1[$. La moyenne empirique, $\hat{p}_n = (X_1 + \dotsb + X_n)/n$ est un estimateur est non biaisé de $p$ et son risque quadratique $p(1-p)/n$. De plus, la loi de $\hat{p}_n$ est connue : $n\hat{p}_n \sim \mathrm{Bin}(n,p)$. Par conséquent, si l'on connaît les quantiles de $\mathrm{Bin}(n,p)-p$, on pourra construire des intervalles de confiance de niveau $1-\alpha$. Ces quantiles peuvent être calculés par des méthodes numériques, mais il y a plus simple. 

**Inégalité BT. ** L'inégalité de Bienaymé-Tchebychev dit que 
$$P_p(|\hat{p}_n - p|>t)\leqslant \frac{p(1-p)}{nt^2}. $${#eq-btbin}
Si l'on choisit $$t = \sqrt{\frac{p(1-p)}{n\alpha}},$$ cette probabilité est plus petite que $\alpha$. En pivotant, on en déduit que l'intervalle $[\hat{p_n} \pm \sqrt{p(1-p)/n\alpha}]$ contient $p$ avec une probabilité supérieure à $1-\alpha$. Mais les bornes de cet intervalle ne sont pas des statistiques, car elles dépendent de $p$ ! Fort heureusement, on sait que $p$ est entre $0$ et $1$, ce qui entraîne que $p(1-p)$ est plus petit que $1/4$, donc l'intervalle ci-dessus est contenu dans l'intervalle plus grand
$$ \left[\hat{p}_n \pm \frac{1}{2\sqrt{n\alpha}}\right]. $$
Ce dernier est bien un intervalle de confiance de niveau $1-\alpha$ pour l'estimation de $p$. 

**TCL. ** On a mentionné que les quantiles des lois binomiales pourraient être calculés ; or, ils peuvent également être approchés grâce au théorème central-limite. Celui-ci dit que 
$$ \frac{\sqrt{n}(\hat{p}_n - p)}{\sqrt{p(1-p)}} \to N(0,1).$${#eq-tclbin}
Si $z_\alpha$ est le quantile symétrique d'ordre $\alpha$ de $N(0,1)$, alors on en déduit que 
$$\mathbb{P}\left(\left|\frac{\sqrt{n}(\hat{p}_n - p)}{\sqrt{p(1-p)}} \right|>z_\alpha \right) \to \alpha. $$
En pivotant, on voit alors que l'intervalle 
$$\left[\hat{p}_n \pm z_\alpha \sqrt{p(1-p)/n}\right] $$
contient $p$ avec une probabilité *qui tend lorsque $n\to\infty$ vers $1-\alpha$*. Là encore, cet intervalle n'est pas un intervalle de confiance. On pourrait utiliser deux techniques. 

1. Comme tout à l'heure, l'intervalle ci-dessus est contenu dans l'intervalle plus grand $[\hat{p}_n \pm z_\alpha/2\sqrt{n}]$ qui est un intervalle de confiance *asymptotique* de niveau $1-\alpha$. 
2. Il y a plus fin. Nous savons par la loi des grands nombres que $\hat{p}_n \to p$ en probabilité. Ainsi, $\sqrt{\hat{p}_n(1-\hat{p}_n)} \to \sqrt{p(1-p)}$ en probabilité. Le lemme de Slutsky nous assure alors que dans @eq-tclbin, on peut remplacer le dénominateur par $\sqrt{\hat{p}_n (1-\hat{p}_n)}$ pour obtenir
$$ \frac{\sqrt{n}(\hat{p}_n - p)}{\sqrt{\hat{p}_n(1-\hat{p}_n)}} \to N(0,1).$$
Le reste du raisonnement est identique, et l'on obtient l'intervalle de confiance asymptotique de niveau $1-\alpha$ suivant : 
$$\left[\hat{p}_n \pm z_\alpha \sqrt{\frac{\hat{p}_n(1-\hat{p}_n)}{n}}\right] $$



**Hoeffding.** L'inégalité de Bienaymé-Tchebychev n'est pas très fine. Il existe de nombreuses autres inégalités de concentration : l'inégalité de Hoeffding (@thm-hoeffding) concerne les variables bornées, comme ici où les $X_i$ sont dans $[0,1]$ . Cette inégalité dit que 
$$\mathbb{P}(|\hat{p}_n - p|>t)\leqslant 2 e^{-2nt^2}. $$ 
Le choix
$$ t = \sqrt{\frac{1}{2n}\ln\left(\frac{2}{\alpha}\right)}$$ donne une probabilité inférieure à $\alpha$, et fournit donc l'intervalle de confiance **non-asymptotique** de niveau $1-\alpha$ suivant : 
$$ \left[\bar{X}_n \pm \frac{\ln(2/\alpha)}{\sqrt{2n}}\right].$$

### Estimation de moyenne dans un modèle non-gaussien. 

Les deux techniques ci-dessus n'ont rien de spécifique au cas de variables de Bernoulli. En fait, elles s'appliquent à tout modèle statistique iid dont on cherche à estimer la moyenne $\mu$, pourvu que la variance existe. 

La première méthode utilisant Bienaymé-Tchebychev nécessite de borner la variance. Cela peut se faire dans certains cas, mais pas dans tous. L'inégalité de Hoeffding est beaucoup plus fine que l'inégalité de Bienaymé-Tchebychev, mais elle ne s'applique qu'aux variables qui sont bornées ou sous-gaussiennes. 

La seconde méthode s'applique systématiquement en utilisant l'estimateur de la variance empirique $\hat{\sigma}_n^2$. En effet, la convergence 
$$\frac{\sqrt{n}}{\hat{\sigma}_n}(\bar{X}_n - \mu) \to N(0,1)$$
est toujours vraie d'après le théorème de Slutsky. 

:::{#thm-icasymptcl}

Soient $X_1, \dotsc, X_n$ des variables iid possédant une variance. L'intervalle 
$$ \left[ \bar{X}_n \pm \frac{z_\alpha \hat{\sigma}_n}{\sqrt{n}} \right]$$
est un intervalle de confiance asymptotique de niveau $\alpha$ pour l'estimation de la moyenne des $X_i$. 

:::